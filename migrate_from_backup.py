#!/usr/bin/env python3
"""
Rebuild the PostgreSQL database schema and data from an Excel backup.

Usage:
    python migrate_from_backup.py --backup ./scripts/backups/backup_YYYYMMDD_HHMMSS.xlsx

Steps performed:
    1. Connect to the configured PostgreSQL database.
    2. Drop existing tables related to the label system (if any).
    3. Recreate schema (labels, feedback_sentiments, feedback_intents, triggers, indexes).
    4. Import data from the provided Excel backup file.

Requirements:
    pip install pandas openpyxl psycopg2-binary python-dotenv httpx
"""
from __future__ import annotations

import argparse
import sys
from datetime import datetime
from pathlib import Path
from typing import Optional

import pandas as pd
import psycopg2
from psycopg2.extras import execute_batch
from dotenv import load_dotenv
import httpx
import os

# Load environment variables if .env exists
load_dotenv()

# Database configuration (align with other scripts)
DB_HOST = "localhost"
DB_PORT = 5499
DB_NAME = "label_db"
DB_USER = "postgres"
DB_PASSWORD = "qwertyxxx"


def get_connection() -> psycopg2.extensions.connection:
    """Create a psycopg2 connection."""
    try:
        conn = psycopg2.connect(
            host=DB_HOST,
            port=DB_PORT,
            dbname=DB_NAME,
            user=DB_USER,
            password=DB_PASSWORD,
        )
        conn.autocommit = False
        return conn
    except Exception as exc:  # pragma: no cover - fatal
        print(f"âŒ KhÃ´ng thá»ƒ káº¿t ná»‘i database: {exc}", file=sys.stderr)
        print(f"   Host: {DB_HOST}:{DB_PORT}, DB: {DB_NAME}, User: {DB_USER}", file=sys.stderr)
        raise


def find_latest_backup(default_dir: Path) -> Path:
    """Find the most recent backup_xxx.xlsx in the directory."""
    if not default_dir.exists():
        raise FileNotFoundError(f"ThÆ° má»¥c backup khÃ´ng tá»“n táº¡i: {default_dir}")

    backups = sorted(default_dir.glob("backup_*.xlsx"))
    if not backups:
        raise FileNotFoundError(f"KhÃ´ng tÃ¬m tháº¥y file backup trong {default_dir}")
    return backups[-1]


def recreate_schema(conn: psycopg2.extensions.connection) -> None:
    """Drop existing tables and recreate schema."""
    with conn.cursor() as cur:
        print("ğŸ—„ï¸  Äang khá»Ÿi táº¡o schema...")

        # Enable uuid extension
        cur.execute('CREATE EXTENSION IF NOT EXISTS "uuid-ossp";')

        # Drop tables in dependency order
        cur.execute("DROP TABLE IF EXISTS feedback_intents CASCADE;")
        cur.execute("DROP TABLE IF EXISTS feedback_sentiments CASCADE;")
        cur.execute("DROP TABLE IF EXISTS labels CASCADE;")

        # Recreate labels table
        cur.execute(
            """
            CREATE TABLE labels (
                id INTEGER PRIMARY KEY,
                name VARCHAR(255) NOT NULL,
                level INTEGER NOT NULL CHECK (level IN (1, 2, 3)),
                parent_id INTEGER REFERENCES labels(id) ON DELETE CASCADE,
                description TEXT,
                created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
                embedding REAL[]
            );
            """
        )

        # Indexes for labels
        cur.execute("CREATE INDEX idx_labels_parent_id ON labels(parent_id);")
        cur.execute("CREATE INDEX idx_labels_level ON labels(level);")
        cur.execute("CREATE INDEX idx_labels_name ON labels(name);")
        cur.execute("CREATE INDEX idx_labels_created_at ON labels(created_at DESC);")
        cur.execute("CREATE INDEX idx_labels_embedding ON labels USING GIN(embedding);")

        # Update trigger for updated_at
        cur.execute(
            """
            CREATE OR REPLACE FUNCTION update_updated_at_column()
            RETURNS TRIGGER AS $$
            BEGIN
                NEW.updated_at = CURRENT_TIMESTAMP;
                RETURN NEW;
            END;
            $$ language 'plpgsql';
            """
        )

        cur.execute(
            """
            CREATE TRIGGER update_labels_updated_at
                BEFORE UPDATE ON labels
                FOR EACH ROW
                EXECUTE FUNCTION update_updated_at_column();
            """
        )

        # Recreate feedback_sentiments
        cur.execute(
            """
            CREATE TABLE feedback_sentiments (
                id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
                feedback_text TEXT NOT NULL,
                sentiment_label VARCHAR(50) NOT NULL,
                confidence_score REAL NOT NULL,
                feedback_source VARCHAR(50) NOT NULL,
                is_model_confirmed BOOLEAN NOT NULL DEFAULT FALSE,
                created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
                level1_id INTEGER REFERENCES labels(id) ON DELETE SET NULL,
                level2_id INTEGER REFERENCES labels(id) ON DELETE SET NULL,
                level3_id INTEGER REFERENCES labels(id) ON DELETE SET NULL
            );
            """
        )

        cur.execute("CREATE INDEX idx_feedback_sentiments_source ON feedback_sentiments(feedback_source);")
        cur.execute("CREATE INDEX idx_feedback_sentiments_label ON feedback_sentiments(sentiment_label);")
        cur.execute("CREATE INDEX idx_feedback_sentiments_created_at ON feedback_sentiments(created_at DESC);")
        cur.execute("CREATE INDEX idx_feedback_sentiments_level1 ON feedback_sentiments(level1_id);")
        cur.execute("CREATE INDEX idx_feedback_sentiments_level2 ON feedback_sentiments(level2_id);")
        cur.execute("CREATE INDEX idx_feedback_sentiments_level3 ON feedback_sentiments(level3_id);")

        # Recreate feedback_intents
        cur.execute(
            """
            CREATE TABLE feedback_intents (
                id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
                feedback_id UUID NOT NULL REFERENCES feedback_sentiments(id) ON DELETE CASCADE,
                level1_id INTEGER NOT NULL REFERENCES labels(id) ON DELETE CASCADE,
                level2_id INTEGER NOT NULL REFERENCES labels(id) ON DELETE CASCADE,
                level3_id INTEGER NOT NULL REFERENCES labels(id) ON DELETE CASCADE,
                avg_cosine_similarity REAL NOT NULL,
                created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
                UNIQUE (feedback_id, level1_id, level2_id, level3_id)
            );
            """
        )

        cur.execute("CREATE INDEX idx_feedback_intents_feedback_id ON feedback_intents(feedback_id);")
        cur.execute("CREATE INDEX idx_feedback_intents_level1_id ON feedback_intents(level1_id);")
        cur.execute("CREATE INDEX idx_feedback_intents_level2_id ON feedback_intents(level2_id);")
        cur.execute("CREATE INDEX idx_feedback_intents_level3_id ON feedback_intents(level3_id);")
        cur.execute("CREATE INDEX idx_feedback_intents_similarity ON feedback_intents(avg_cosine_similarity DESC);")
        cur.execute("CREATE INDEX idx_feedback_intents_created_at ON feedback_intents(created_at DESC);")

        print("âœ… Schema Ä‘Ã£ Ä‘Æ°á»£c táº¡o láº¡i thÃ nh cÃ´ng.")


def read_sheet(excel_path: Path, sheet_name: str) -> Optional[pd.DataFrame]:
    """Read a sheet as DataFrame if it exists."""
    try:
        return pd.read_excel(excel_path, sheet_name=sheet_name)
    except ValueError:
        # Sheet not found
        return None


def import_labels(conn: psycopg2.extensions.connection, df: pd.DataFrame) -> int:
    """Import labels into database."""
    if df is None or df.empty:
        raise ValueError("Sheet 'labels' khÃ´ng cÃ³ dá»¯ liá»‡u, khÃ´ng thá»ƒ khÃ´i phá»¥c.")

    required_columns = {"id", "name", "level", "parent_id", "description"}
    missing = required_columns - set(df.columns)
    if missing:
        raise ValueError(f"Sheet 'labels' thiáº¿u cÃ¡c cá»™t báº¯t buá»™c: {', '.join(sorted(missing))}")

    records = []
    for row in df.itertuples(index=False):
        parent_id = getattr(row, "parent_id")
        parent_id = None if pd.isna(parent_id) else int(parent_id)
        description = getattr(row, "description")
        description = None if pd.isna(description) else description
        records.append(
            (
                int(getattr(row, "id")),
                str(getattr(row, "name")),
                int(getattr(row, "level")),
                parent_id,
                description,
            )
        )

    sql = """
        INSERT INTO labels (id, name, level, parent_id, description)
        VALUES (%s, %s, %s, %s, %s)
        ON CONFLICT (id) DO UPDATE
            SET name = EXCLUDED.name,
                level = EXCLUDED.level,
                parent_id = EXCLUDED.parent_id,
                description = EXCLUDED.description
    """

    with conn.cursor() as cur:
        execute_batch(cur, sql, records, page_size=200)

    return len(records)


def import_feedback_sentiments(conn: psycopg2.extensions.connection, df: Optional[pd.DataFrame]) -> int:
    """Import feedback_sentiments if sheet exists."""
    if df is None or df.empty:
        print("â„¹ï¸  Sheet 'feedback_sentiments' khÃ´ng cÃ³ dá»¯ liá»‡u, skip.")
        return 0

    required_columns = {
        "id",
        "feedback_text",
        "sentiment_label",
        "confidence_score",
        "feedback_source",
        "level1_id",
        "level2_id",
        "level3_id",
    }
    available_columns = set(df.columns)
    missing = required_columns - available_columns
    if missing:
        raise ValueError(f"Sheet 'feedback_sentiments' thiáº¿u cá»™t: {', '.join(sorted(missing))}")

    # Ensure confirmation column exists
    if "is_model_confirmed" not in available_columns:
        df = df.copy()
        df["is_model_confirmed"] = False

    records = []
    for row in df.itertuples(index=False):
        def normalize(value, cast_type=None):
            if pd.isna(value):
                return None
            if cast_type:
                return cast_type(value)
            return value

        is_confirmed = getattr(row, "is_model_confirmed")
        if pd.isna(is_confirmed):
            is_confirmed = False
        else:
            is_confirmed = bool(is_confirmed)

        records.append(
            (
                str(getattr(row, "id")),
                str(getattr(row, "feedback_text")),
                str(getattr(row, "sentiment_label")),
                float(getattr(row, "confidence_score")),
                str(getattr(row, "feedback_source")),
                normalize(getattr(row, "level1_id"), int),
                normalize(getattr(row, "level2_id"), int),
                normalize(getattr(row, "level3_id"), int),
                is_confirmed,
            )
        )

    sql = """
        INSERT INTO feedback_sentiments (
            id,
            feedback_text,
            sentiment_label,
            confidence_score,
            feedback_source,
            level1_id,
            level2_id,
            level3_id,
            is_model_confirmed
        )
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
        ON CONFLICT (id) DO UPDATE
            SET feedback_text = EXCLUDED.feedback_text,
                sentiment_label = EXCLUDED.sentiment_label,
                confidence_score = EXCLUDED.confidence_score,
                feedback_source = EXCLUDED.feedback_source,
                level1_id = EXCLUDED.level1_id,
                level2_id = EXCLUDED.level2_id,
                level3_id = EXCLUDED.level3_id,
                is_model_confirmed = EXCLUDED.is_model_confirmed
    """

    with conn.cursor() as cur:
        execute_batch(cur, sql, records, page_size=200)

    return len(records)


def seed_label_embeddings(conn: psycopg2.extensions.connection) -> None:
    """Seed embeddings cho táº¥t cáº£ labels sau khi import."""
    # Láº¥y embedding service URL tá»« env hoáº·c thá»­ cÃ¡c URL phá»• biáº¿n
    embedding_service_url = os.getenv("EMBEDDING_SERVICE_URL")
    
    # Thá»­ cÃ¡c URL phá»• biáº¿n náº¿u khÃ´ng cÃ³ trong env
    possible_urls = [
        embedding_service_url,
        "http://embedding-service:8000/api/v1",  # Docker network
        "http://localhost:8000/api/v1",  # Local
    ]
    
    # TÃ¬m URL hoáº¡t Ä‘á»™ng
    working_url = None
    for url in possible_urls:
        if not url:
            continue
        try:
            with httpx.Client(timeout=5.0) as client:
                # Test connection vá»›i health check endpoint
                # URL cÃ³ thá»ƒ lÃ  http://embedding-service:8000/api/v1 hoáº·c http://localhost:8000/api/v1
                base_url = url.replace('/encode', '').rstrip('/')
                health_url = f"{base_url}/health"
                test_response = client.get(health_url, timeout=5.0)
                if test_response.status_code == 200:
                    working_url = url
                    break
        except Exception:
            continue
    
    if not working_url:
        print("   âš ï¸  KhÃ´ng thá»ƒ káº¿t ná»‘i Ä‘áº¿n embedding service.")
        print("   ğŸ’¡ CÃ³ thá»ƒ:")
        print("      - Embedding service chÆ°a cháº¡y")
        print("      - URL khÃ´ng Ä‘Ãºng (kiá»ƒm tra EMBEDDING_SERVICE_URL trong .env)")
        print("      - Náº¿u cháº¡y trong Docker, dÃ¹ng: http://embedding-service:8000/api/v1")
        print("      - Náº¿u cháº¡y local, dÃ¹ng: http://localhost:8000/api/v1")
        print("   ğŸ’¡ Báº¡n cÃ³ thá»ƒ seed embeddings sau báº±ng:")
        print("      - POST /admin/seed-label-embeddings")
        print("      - hoáº·c: python seed_data.py --labels-only")
        return
    
    print(f"   ğŸ”— Káº¿t ná»‘i embedding service: {working_url}")
    
    # Láº¥y táº¥t cáº£ labels
    with conn.cursor() as cur:
        cur.execute("SELECT id, name, description FROM labels ORDER BY level, id")
        labels = cur.fetchall()
    
    if not labels:
        print("   â„¹ï¸  KhÃ´ng cÃ³ labels Ä‘á»ƒ seed embeddings.")
        return
    
    print(f"   ğŸ“‹ TÃ¬m tháº¥y {len(labels)} labels cáº§n seed embeddings...")
    
    processed = 0
    failed = 0
    consecutive_failures = 0
    max_consecutive_failures = 5
    
    for label_id, name, description in labels:
        try:
            # Táº¡o text cho embedding
            text = name
            if description and not pd.isna(description):
                text = f"{name}. {description}"
            
            # Gá»i embedding service
            with httpx.Client(timeout=30.0) as client:
                response = client.post(
                    f"{working_url}/encode",
                    json={"text": text}
                )
                response.raise_for_status()
                result = response.json()
                embedding = result.get("embedding", [])
            
            if not embedding:
                print(f"   âš ï¸  KhÃ´ng nháº­n Ä‘Æ°á»£c embedding cho label {label_id} ({name})")
                failed += 1
                consecutive_failures += 1
                if consecutive_failures >= max_consecutive_failures:
                    print(f"   â›” QuÃ¡ nhiá»u lá»—i liÃªn tiáº¿p ({consecutive_failures}), dá»«ng seed embeddings.")
                    break
                continue
            
            # Update embedding vÃ o database
            with conn.cursor() as cur:
                cur.execute(
                    "UPDATE labels SET embedding = %s WHERE id = %s",
                    (embedding, label_id)
                )
            
            processed += 1
            consecutive_failures = 0  # Reset counter on success
            if processed % 10 == 0:
                print(f"   ğŸ“Š ÄÃ£ xá»­ lÃ½ {processed}/{len(labels)} labels...")
        
        except Exception as e:
            failed += 1
            consecutive_failures += 1
            if consecutive_failures <= 3:  # Chá»‰ hiá»ƒn thá»‹ 3 lá»—i Ä‘áº§u
                print(f"   âš ï¸  Lá»—i khi seed embedding cho label {label_id}: {e}")
            elif consecutive_failures == 4:
                print(f"   âš ï¸  ... (Ä‘ang gáº·p lá»—i liÃªn tiáº¿p)")
            if consecutive_failures >= max_consecutive_failures:
                print(f"   â›” QuÃ¡ nhiá»u lá»—i liÃªn tiáº¿p ({consecutive_failures}), dá»«ng seed embeddings.")
                break
    
    conn.commit()
    if processed > 0:
        print(f"   âœ… ÄÃ£ seed embeddings: {processed} thÃ nh cÃ´ng, {failed} tháº¥t báº¡i")
    else:
        print(f"   âŒ KhÃ´ng thá»ƒ seed embeddings: {failed} tháº¥t báº¡i")


def import_feedback_intents(conn: psycopg2.extensions.connection, df: Optional[pd.DataFrame]) -> int:
    """Import feedback_intents if sheet exists."""
    if df is None or df.empty:
        print("â„¹ï¸  Sheet 'feedback_intents' khÃ´ng cÃ³ dá»¯ liá»‡u, skip.")
        return 0

    required_columns = {
        "id",
        "feedback_id",
        "level1_id",
        "level2_id",
        "level3_id",
        "avg_cosine_similarity",
    }
    missing = required_columns - set(df.columns)
    if missing:
        raise ValueError(f"Sheet 'feedback_intents' thiáº¿u cá»™t: {', '.join(sorted(missing))}")

    records = []
    for row in df.itertuples(index=False):
        records.append(
            (
                str(getattr(row, "id")),
                str(getattr(row, "feedback_id")),
                int(getattr(row, "level1_id")),
                int(getattr(row, "level2_id")),
                int(getattr(row, "level3_id")),
                float(getattr(row, "avg_cosine_similarity")),
            )
        )

    sql = """
        INSERT INTO feedback_intents (
            id,
            feedback_id,
            level1_id,
            level2_id,
            level3_id,
            avg_cosine_similarity
        )
        VALUES (%s, %s, %s, %s, %s, %s)
        ON CONFLICT (id) DO UPDATE
            SET feedback_id = EXCLUDED.feedback_id,
                level1_id = EXCLUDED.level1_id,
                level2_id = EXCLUDED.level2_id,
                level3_id = EXCLUDED.level3_id,
                avg_cosine_similarity = EXCLUDED.avg_cosine_similarity
    """

    with conn.cursor() as cur:
        execute_batch(cur, sql, records, page_size=200)

    return len(records)


def main() -> None:
    parser = argparse.ArgumentParser(description="Rebuild database from Excel backup.")
    parser.add_argument(
        "--backup",
        type=str,
        default=None,
        help="ÄÆ°á»ng dáº«n tá»›i file backup Excel (máº·c Ä‘á»‹nh: láº¥y file má»›i nháº¥t trong ./scripts/backups)",
    )
    parser.add_argument(
        "--backup-dir",
        type=str,
        default="scripts/backups",
        help="ThÆ° má»¥c chá»©a cÃ¡c file backup (dÃ¹ng khi khÃ´ng chá»‰ Ä‘á»‹nh --backup)",
    )
    parser.add_argument(
        "--skip-embeddings",
        action="store_true",
        help="Bá» qua bÆ°á»›c seed embeddings cho labels",
    )
    args = parser.parse_args()

    if args.backup:
        backup_path = Path(args.backup)
    else:
        backup_path = find_latest_backup(Path(args.backup_dir))

    if not backup_path.exists():
        raise FileNotFoundError(f"KhÃ´ng tÃ¬m tháº¥y file backup: {backup_path}")

    print("=" * 70)
    print("  ğŸš€ DATABASE REBUILD FROM BACKUP")
    print("=" * 70)
    print(f"ğŸ“ Backup file: {backup_path.resolve()}")
    print(f"ğŸ•’ Start time : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()

    conn = get_connection()

    try:
        recreate_schema(conn)

        print("ğŸ“¥ Äang Ä‘á»c dá»¯ liá»‡u tá»« backup...")
        labels_df = read_sheet(backup_path, "labels")
        feedbacks_df = read_sheet(backup_path, "feedback_sentiments")
        intents_df = read_sheet(backup_path, "feedback_intents")

        print("ğŸ”„ Äang import labels...")
        label_count = import_labels(conn, labels_df)
        print(f"   âœ… ÄÃ£ import {label_count} labels.")

        print("ğŸ”„ Äang import feedback_sentiments...")
        feedback_count = import_feedback_sentiments(conn, feedbacks_df)
        print(f"   âœ… ÄÃ£ import {feedback_count} feedback sentiments.")

        print("ğŸ”„ Äang import feedback_intents...")
        intent_count = import_feedback_intents(conn, intents_df)
        print(f"   âœ… ÄÃ£ import {intent_count} feedback intents.")

        conn.commit()
        
        # Seed embeddings cho labels sau khi import (náº¿u khÃ´ng skip)
        if not args.skip_embeddings:
            print("\nğŸ”„ Äang seed embeddings cho labels...")
            try:
                seed_label_embeddings(conn)
            except Exception as e:
                print(f"   âš ï¸  Lá»—i khi seed embeddings: {e}")
                print("   ğŸ’¡ Báº¡n cÃ³ thá»ƒ cháº¡y láº¡i sau báº±ng: POST /admin/seed-label-embeddings")
        else:
            print("\nâ­ï¸  Bá» qua seed embeddings (--skip-embeddings Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh)")
            print("   ğŸ’¡ Cháº¡y sau báº±ng: POST /admin/seed-label-embeddings hoáº·c python seed_data.py --labels-only")

        print("\nğŸ‰ HoÃ n táº¥t khÃ´i phá»¥c database!")
    except Exception as exc:
        conn.rollback()
        print(f"\nâŒ Lá»—i trong quÃ¡ trÃ¬nh khÃ´i phá»¥c: {exc}", file=sys.stderr)
        raise
    finally:
        conn.close()
        print("ğŸ”š ÄÃ£ Ä‘Ã³ng káº¿t ná»‘i database.")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nâ›” ÄÃ£ há»§y bá»Ÿi ngÆ°á»i dÃ¹ng.")
        sys.exit(1)
    except Exception:
        sys.exit(1)

